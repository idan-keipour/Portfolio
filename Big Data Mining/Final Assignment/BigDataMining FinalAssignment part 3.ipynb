{"cells":[{"cell_type":"markdown","source":["Qu. 3: Data Analysis and PageRank in Spark [46pt]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe9b4d53-7235-4a34-bdd2-e1586ac22e28"}}},{"cell_type":"code","source":["# Required modules\nimport re\nimport sys\nfrom operator import add\n\n# Set File Paths\ntripdelaysFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\nairportsnaFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n\n# Obtain airports dataset\nairports  = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true', delimiter='\\t').load(airportsnaFilePath)\nairports.registerTempTable(\"airports\")\n\n# Obtain departure Delays data\ndelays = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').load(tripdelaysFilePath)\ndelays.registerTempTable(\"delays\")\ndelays.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a0653e0-e985-4b92-a823-3f58b3521e62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/dataframe.py:146: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n  warnings.warn(\nOut[2]: DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/dataframe.py:146: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n  warnings.warn(\nOut[2]: DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.a. [3pt] Show the top 10 airport and top 10 delays from both dataframes in a nice table format"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81f28bf8-5e9b-49bb-8e30-4c2d4a3aad29"}}},{"cell_type":"code","source":["print(\"The top 10 rows of the delays file:\")\ndelays.show(10,truncate=False)\n\nprint(\"The top 10 rows of the airports file:\")\nairports.show(10,truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a6b04aa-ec37-447a-a009-bf94797da93c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The top 10 rows of the delays file:\n+--------+-----+--------+------+-----------+\n|date    |delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01011245|6    |602     |ABE   |ATL        |\n|01020600|-8   |369     |ABE   |DTW        |\n|01021245|-2   |602     |ABE   |ATL        |\n|01020605|-4   |602     |ABE   |ATL        |\n|01031245|-4   |602     |ABE   |ATL        |\n|01030605|0    |602     |ABE   |ATL        |\n|01041243|10   |602     |ABE   |ATL        |\n|01040605|28   |602     |ABE   |ATL        |\n|01051245|88   |602     |ABE   |ATL        |\n|01050605|9    |602     |ABE   |ATL        |\n+--------+-----+--------+------+-----------+\nonly showing top 10 rows\n\nThe top 10 rows of the airports file:\n+-----------+-----+-------+----+\n|City       |State|Country|IATA|\n+-----------+-----+-------+----+\n|Abbotsford |BC   |Canada |YXX |\n|Aberdeen   |SD   |USA    |ABR |\n|Abilene    |TX   |USA    |ABI |\n|Akron      |OH   |USA    |CAK |\n|Alamosa    |CO   |USA    |ALS |\n|Albany     |GA   |USA    |ABY |\n|Albany     |NY   |USA    |ALB |\n|Albuquerque|NM   |USA    |ABQ |\n|Alexandria |LA   |USA    |AEX |\n|Allentown  |PA   |USA    |ABE |\n+-----------+-----+-------+----+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The top 10 rows of the delays file:\n+--------+-----+--------+------+-----------+\n|date    |delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01011245|6    |602     |ABE   |ATL        |\n|01020600|-8   |369     |ABE   |DTW        |\n|01021245|-2   |602     |ABE   |ATL        |\n|01020605|-4   |602     |ABE   |ATL        |\n|01031245|-4   |602     |ABE   |ATL        |\n|01030605|0    |602     |ABE   |ATL        |\n|01041243|10   |602     |ABE   |ATL        |\n|01040605|28   |602     |ABE   |ATL        |\n|01051245|88   |602     |ABE   |ATL        |\n|01050605|9    |602     |ABE   |ATL        |\n+--------+-----+--------+------+-----------+\nonly showing top 10 rows\n\nThe top 10 rows of the airports file:\n+-----------+-----+-------+----+\n|City       |State|Country|IATA|\n+-----------+-----+-------+----+\n|Abbotsford |BC   |Canada |YXX |\n|Aberdeen   |SD   |USA    |ABR |\n|Abilene    |TX   |USA    |ABI |\n|Akron      |OH   |USA    |CAK |\n|Alamosa    |CO   |USA    |ALS |\n|Albany     |GA   |USA    |ABY |\n|Albany     |NY   |USA    |ALB |\n|Albuquerque|NM   |USA    |ABQ |\n|Alexandria |LA   |USA    |AEX |\n|Allentown  |PA   |USA    |ABE |\n+-----------+-----+-------+----+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.b. [9pt] Run sql commands to answer the following questions:\n\n(i) What US city incurs the most delays as an origin airport? Run an sql query to find out."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6910134-488a-41d4-a605-2cf9836d3001"}}},{"cell_type":"code","source":["%sql\nSELECT origin,City, count(origin) as number_of_delays FROM (SELECT t1.origin, t2.City FROM delays t1 JOIN airports t2 on t2.IATA = t1.origin) GROUP by origin,City order by number_of_delays desc limit 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97baa0ea-fc8f-4bc2-ad4c-4ec24af51af1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["ATL","Atlanta",91484]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"origin","type":"\"string\"","metadata":"{}"},{"name":"City","type":"\"string\"","metadata":"{}"},{"name":"number_of_delays","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>origin</th><th>City</th><th>number_of_delays</th></tr></thead><tbody><tr><td>ATL</td><td>Atlanta</td><td>91484</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["(ii) We would like to know, for each origin airport and state combination,\nthe average distance and delay of all outgoing flights with positive delays.\nIn addition, in the same query, we would like to see the average state delay\n(regardless of origin airport) for each airport-state combination with positive delay.\n\nWrite an sql query that returns origin, state, and also:\naverage distance (mean distance between origin and destinations over all outgoing flights)\naverage delay (average delay of all outgoing flights from an origin, with a positive delay)\naverage state delay (average dealy over all outgloing flights from all airports in the same state, with a positive delay).\n\nThe query should return the results sorted by decreasing average state delay, with only the first 10 rows shown.\n\nHint: use a window function among other sql commands."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3b9bfab-4f48-4990-b92c-e7836f4e3c9d"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TABLE with_state SELECT t1.origin, t1.distance,t1.delay,t2.State FROM delays t1 JOIN airports t2 on t2.IATA = t1.origin WHERE \nt1.delay > 0;\n\nSELECT distinct  origin, State, ROUND (AVG(distance) OVER (PARTITION BY origin),2) AS avg_airport_distance,\n                ROUND (AVG(delay) OVER (PARTITION BY origin),2) AS avg_airport_delay,\n                ROUND (AVG(delay) OVER (PARTITION BY State),2) AS avg_state_delay\n                FROM with_state  ORDER BY avg_state_delay desc limit 10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e13eea75-f51a-4a92-99fc-34e427e140f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["BTV","VT",408.02,67.78,67.78],["RAP","SD",471.18,43.48,49.86],["FSD","SD",415.05,52.22,49.86],["BGR","ME",467.62,70.94,49.74],["PWM","ME",403.31,46.61,49.74],["ORF","VA",462.26,45.04,48.53],["CHO","VA",399.15,55.2,48.53],["ROA","VA",328.12,68.04,48.53],["RIC","VA",476.9,48.74,48.53],["PHF","VA",458.28,46.55,48.53]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"origin","type":"\"string\"","metadata":"{}"},{"name":"State","type":"\"string\"","metadata":"{}"},{"name":"avg_airport_distance","type":"\"double\"","metadata":"{}"},{"name":"avg_airport_delay","type":"\"double\"","metadata":"{}"},{"name":"avg_state_delay","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>origin</th><th>State</th><th>avg_airport_distance</th><th>avg_airport_delay</th><th>avg_state_delay</th></tr></thead><tbody><tr><td>BTV</td><td>VT</td><td>408.02</td><td>67.78</td><td>67.78</td></tr><tr><td>RAP</td><td>SD</td><td>471.18</td><td>43.48</td><td>49.86</td></tr><tr><td>FSD</td><td>SD</td><td>415.05</td><td>52.22</td><td>49.86</td></tr><tr><td>BGR</td><td>ME</td><td>467.62</td><td>70.94</td><td>49.74</td></tr><tr><td>PWM</td><td>ME</td><td>403.31</td><td>46.61</td><td>49.74</td></tr><tr><td>ORF</td><td>VA</td><td>462.26</td><td>45.04</td><td>48.53</td></tr><tr><td>CHO</td><td>VA</td><td>399.15</td><td>55.2</td><td>48.53</td></tr><tr><td>ROA</td><td>VA</td><td>328.12</td><td>68.04</td><td>48.53</td></tr><tr><td>RIC</td><td>VA</td><td>476.9</td><td>48.74</td><td>48.53</td></tr><tr><td>PHF</td><td>VA</td><td>458.28</td><td>46.55</td><td>48.53</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["(iii) We would like to compute the PageRank vector only for origin nodes that have outgoing edges.\nRemove from the delays dataframe all the records of flights connecting to destination aiports that\nappear only as destination (also called dead-end nodes, or dangling page).\nThat is, if a record contains as origin an airport called ABC, and as destination an airport called XYZ,\nyou should keep it only if XYZ is an origin airport for another record.\nUse an sql command inside python using Spark.sql, and update the delays variable to contain the output."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97d68c26-1d61-4882-b511-6ab73b61b363"}}},{"cell_type":"code","source":["print(\"Before removal there were \"+str(delays.count())+\" rows\")\ndelays = spark.sql(\"select * from delays as orig where exists (select origin from delays as dest where dest.origin = orig.destination)\")\nprint(\"After removal \"+str(delays.count())+\" rows left\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3a8aedc-3eb5-4e54-bee0-317297bb992e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Before removal there were 1391578 rows\nAfter removal 1377301 rows left\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Before removal there were 1391578 rows\nAfter removal 1377301 rows left\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.c[6 pt] In this question we build an object representing the network of delayed flights connecting between airports, as a preparation for the PageRank algorithm.\nFor this, we will only look at the origin and destination columns, not giving any weight to the delay time.\nEach (origin, destination) pair should have at most one link in the the network, even if there are multipled delayed flights connecting them.\n\nCreate a new RDD data structure of tuples called ranks, storing the initial PageRank value for each origin node. Set the initialization value as 1/n for all nodes, where n is the number of origin node.\nWe will ignore nodes that are only destination and do no not appear as origin, in order to avoid dangling pages and to simplify the calculations.\n\nNext, create another RDD data structure of tuples, named links, where each tuple is composed of the origin and an iterator (a GroupByKey) of the destination.\nShow the first  10  rows of the resulting links and ranks RDD data structures."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6f9a732-2896-4fb7-8dc0-3b8be7dff661"}}},{"cell_type":"code","source":["from pyspark.sql import functions\nimport pyspark.sql.functions as F\n\norigins = spark.sql(\"select distinct origin from delays\")\nn = origins.count()\nranks = origins.rdd.map(lambda orig: (orig[0], 1/n))\n\ngrouped = ((spark.sql(\"select distinct origin,destination  from delays\")).rdd.map(lambda orig: (orig[0], orig[1]))).groupByKey()\nlinks = grouped.map(lambda orig: (orig[0], list(orig[1])))\n\nprint(\"First 10 rows of the links file:\")\nprint(links.take(10))\nprint(\"\")\nprint(\"First 10 rows of the ranks file:\")\nprint(ranks.take(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37fc8674-79af-4058-9629-3bb0dc4a4a54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"First 10 rows of the links file:\n[('ATL', ['GSP', 'HDN', 'ALB', 'SRQ', 'BZN', 'GRB', 'LGA', 'PIT', 'AEX', 'ORD', 'TUL', 'CHA', 'GTR', 'MEM', 'ASE', 'CAK', 'ILM', 'DSM', 'IAH', 'PVD', 'CLT', 'MHT', 'EVV', 'ABE', 'AUS', 'CID', 'AGS', 'GSO', 'DEN', 'CRW', 'BQK', 'SJU', 'CVG', 'ROC', 'EYW', 'BMI', 'PNS', 'SFO', 'DAB', 'LAS', 'MCI', 'HNL', 'ECP', 'DHN', 'MSN', 'FSM', 'CSG', 'HPN', 'MCO', 'JFK', 'TYS', 'PHX', 'SAT', 'ATW', 'PHL', 'AVL', 'EWN', 'BDL', 'MLI', 'SAV', 'MTJ', 'MIA', 'TLH', 'ORF', 'COS', 'RDU', 'MSP', 'CMH', 'SNA', 'FAR', 'SLC', 'EGE', 'DTW', 'BUF', 'PBI', 'JAX', 'MDT', 'FAY', 'MOB', 'SYR', 'BOS', 'GRR', 'RSW', 'SEA', 'STL', 'GNV', 'PDX', 'ABQ', 'OMA', 'ABY', 'STT', 'MKE', 'BHM', 'FLL', 'DCA', 'CLE', 'FWA', 'MSY', 'CAE', 'LEX', 'LAX', 'SJC', 'MYR', 'AVP', 'XNA', 'VPS', 'TPA', 'JAN', 'SDF', 'HOU', 'EWR', 'ROA', 'SMF', 'SHV', 'DAL', 'PWM', 'SAN', 'PIA', 'GPT', 'LFT', 'CHO', 'BWI', 'MLU', 'MDW', 'ELP', 'ONT', 'OAJ', 'GRK', 'FSD', 'BNA', 'TRI', 'FNT', 'SBN', 'ICT', 'PHF', 'TUS', 'MLB', 'HSV', 'IAD', 'MGM', 'SGF', 'OKC', 'LIT', 'DAY', 'RIC', 'IND', 'JAC', 'BTR', 'CHS', 'DFW', 'TTN', 'BTV', 'VLD', 'PSP']), ('BQN', ['MCO', 'EWR', 'JFK']), ('CLE', ['SJU', 'MCI', 'DEN', 'IAH', 'SEA', 'SYR', 'GSP', 'IND', 'PHL', 'PBI', 'LGA', 'IAD', 'ATL', 'BTV', 'CHS', 'PHX', 'BWI', 'GRR', 'CMH', 'ROC', 'BOS', 'MDW', 'RDU', 'DTW', 'MHT', 'ORD', 'DFW', 'SAN', 'AUS', 'EWR', 'PIT', 'ALB', 'MSN', 'RIC', 'MCO', 'LAX', 'JFK', 'OKC', 'CLT', 'DAY', 'SFO', 'PVD', 'MSP', 'RSW', 'BDL', 'TPA', 'MSY', 'SDF', 'MKE', 'FLL', 'LAS', 'DCA', 'BNA', 'BUF', 'STL', 'MIA', 'TTN']), ('DSM', ['EWR', 'IAH', 'ATL', 'LAS', 'MDW', 'DFW', 'ORD', 'MSP', 'DEN', 'PHX', 'LGA']), ('EWR', ['STT', 'CMH', 'TPA', 'DAY', 'BWI', 'SAV', 'JAX', 'FLL', 'STL', 'SAN', 'SLC', 'JAC', 'MDT', 'PHX', 'CHS', 'MEM', 'IAH', 'AVL', 'CAE', 'LAX', 'PDX', 'SEA', 'CLE', 'DSM', 'DEN', 'DCA', 'AUS', 'ORD', 'MSY', 'DTW', 'ORF', 'TYS', 'MSP', 'PIT', 'MHT', 'IND', 'HDN', 'CLT', 'BTV', 'TUL', 'HOU', 'SDF', 'IAD', 'XNA', 'ROC', 'PWM', 'BNA', 'SFO', 'ATL', 'MCO', 'PVD', 'BOS', 'BQN', 'DFW', 'LAS', 'BZN', 'RSW', 'GSP', 'RDU', 'MTJ', 'GRR', 'ALB', 'SNA', 'CVG', 'GSO', 'EGE', 'SYR', 'MIA', 'MDW', 'SJU', 'RIC', 'BUF', 'PHL', 'OKC', 'MSN', 'BDL', 'PBI', 'MCI', 'SAT', 'OMA', 'HNL', 'MKE', 'AVP']), ('FSD', ['ATL', 'DEN', 'ORD', 'MSP', 'DFW']), ('AUS', ['ELP', 'CLE', 'IAD', 'OAK', 'BNA', 'ORD', 'LAS', 'DFW', 'CLT', 'BOS', 'SJC', 'TPA', 'ATL', 'FLL', 'MSY', 'HOU', 'SLC', 'PHL', 'MDW', 'PHX', 'SFO', 'EWR', 'SAN', 'IAH', 'DAL', 'HRL', 'LAX', 'JFK', 'LGB', 'SEA', 'DTW', 'BWI', 'DEN', 'DCA', 'LBB', 'MCO', 'MSP']), ('BMI', ['MCO', 'DEN', 'ATL', 'ORD', 'DFW']), ('CAE', ['ATL', 'IAD', 'EWR', 'IAH', 'ORD', 'LGA', 'DFW', 'DTW']), ('CPR', ['DEN', 'SLC'])]\n\nFirst 10 rows of the ranks file:\n[('GEG', 0.00392156862745098), ('BUR', 0.00392156862745098), ('GRB', 0.00392156862745098), ('GTF', 0.00392156862745098), ('GRR', 0.00392156862745098), ('EUG', 0.00392156862745098), ('GSO', 0.00392156862745098), ('BTM', 0.00392156862745098), ('COD', 0.00392156862745098), ('FAR', 0.00392156862745098)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["First 10 rows of the links file:\n[('ATL', ['GSP', 'HDN', 'ALB', 'SRQ', 'BZN', 'GRB', 'LGA', 'PIT', 'AEX', 'ORD', 'TUL', 'CHA', 'GTR', 'MEM', 'ASE', 'CAK', 'ILM', 'DSM', 'IAH', 'PVD', 'CLT', 'MHT', 'EVV', 'ABE', 'AUS', 'CID', 'AGS', 'GSO', 'DEN', 'CRW', 'BQK', 'SJU', 'CVG', 'ROC', 'EYW', 'BMI', 'PNS', 'SFO', 'DAB', 'LAS', 'MCI', 'HNL', 'ECP', 'DHN', 'MSN', 'FSM', 'CSG', 'HPN', 'MCO', 'JFK', 'TYS', 'PHX', 'SAT', 'ATW', 'PHL', 'AVL', 'EWN', 'BDL', 'MLI', 'SAV', 'MTJ', 'MIA', 'TLH', 'ORF', 'COS', 'RDU', 'MSP', 'CMH', 'SNA', 'FAR', 'SLC', 'EGE', 'DTW', 'BUF', 'PBI', 'JAX', 'MDT', 'FAY', 'MOB', 'SYR', 'BOS', 'GRR', 'RSW', 'SEA', 'STL', 'GNV', 'PDX', 'ABQ', 'OMA', 'ABY', 'STT', 'MKE', 'BHM', 'FLL', 'DCA', 'CLE', 'FWA', 'MSY', 'CAE', 'LEX', 'LAX', 'SJC', 'MYR', 'AVP', 'XNA', 'VPS', 'TPA', 'JAN', 'SDF', 'HOU', 'EWR', 'ROA', 'SMF', 'SHV', 'DAL', 'PWM', 'SAN', 'PIA', 'GPT', 'LFT', 'CHO', 'BWI', 'MLU', 'MDW', 'ELP', 'ONT', 'OAJ', 'GRK', 'FSD', 'BNA', 'TRI', 'FNT', 'SBN', 'ICT', 'PHF', 'TUS', 'MLB', 'HSV', 'IAD', 'MGM', 'SGF', 'OKC', 'LIT', 'DAY', 'RIC', 'IND', 'JAC', 'BTR', 'CHS', 'DFW', 'TTN', 'BTV', 'VLD', 'PSP']), ('BQN', ['MCO', 'EWR', 'JFK']), ('CLE', ['SJU', 'MCI', 'DEN', 'IAH', 'SEA', 'SYR', 'GSP', 'IND', 'PHL', 'PBI', 'LGA', 'IAD', 'ATL', 'BTV', 'CHS', 'PHX', 'BWI', 'GRR', 'CMH', 'ROC', 'BOS', 'MDW', 'RDU', 'DTW', 'MHT', 'ORD', 'DFW', 'SAN', 'AUS', 'EWR', 'PIT', 'ALB', 'MSN', 'RIC', 'MCO', 'LAX', 'JFK', 'OKC', 'CLT', 'DAY', 'SFO', 'PVD', 'MSP', 'RSW', 'BDL', 'TPA', 'MSY', 'SDF', 'MKE', 'FLL', 'LAS', 'DCA', 'BNA', 'BUF', 'STL', 'MIA', 'TTN']), ('DSM', ['EWR', 'IAH', 'ATL', 'LAS', 'MDW', 'DFW', 'ORD', 'MSP', 'DEN', 'PHX', 'LGA']), ('EWR', ['STT', 'CMH', 'TPA', 'DAY', 'BWI', 'SAV', 'JAX', 'FLL', 'STL', 'SAN', 'SLC', 'JAC', 'MDT', 'PHX', 'CHS', 'MEM', 'IAH', 'AVL', 'CAE', 'LAX', 'PDX', 'SEA', 'CLE', 'DSM', 'DEN', 'DCA', 'AUS', 'ORD', 'MSY', 'DTW', 'ORF', 'TYS', 'MSP', 'PIT', 'MHT', 'IND', 'HDN', 'CLT', 'BTV', 'TUL', 'HOU', 'SDF', 'IAD', 'XNA', 'ROC', 'PWM', 'BNA', 'SFO', 'ATL', 'MCO', 'PVD', 'BOS', 'BQN', 'DFW', 'LAS', 'BZN', 'RSW', 'GSP', 'RDU', 'MTJ', 'GRR', 'ALB', 'SNA', 'CVG', 'GSO', 'EGE', 'SYR', 'MIA', 'MDW', 'SJU', 'RIC', 'BUF', 'PHL', 'OKC', 'MSN', 'BDL', 'PBI', 'MCI', 'SAT', 'OMA', 'HNL', 'MKE', 'AVP']), ('FSD', ['ATL', 'DEN', 'ORD', 'MSP', 'DFW']), ('AUS', ['ELP', 'CLE', 'IAD', 'OAK', 'BNA', 'ORD', 'LAS', 'DFW', 'CLT', 'BOS', 'SJC', 'TPA', 'ATL', 'FLL', 'MSY', 'HOU', 'SLC', 'PHL', 'MDW', 'PHX', 'SFO', 'EWR', 'SAN', 'IAH', 'DAL', 'HRL', 'LAX', 'JFK', 'LGB', 'SEA', 'DTW', 'BWI', 'DEN', 'DCA', 'LBB', 'MCO', 'MSP']), ('BMI', ['MCO', 'DEN', 'ATL', 'ORD', 'DFW']), ('CAE', ['ATL', 'IAD', 'EWR', 'IAH', 'ORD', 'LGA', 'DFW', 'DTW']), ('CPR', ['DEN', 'SLC'])]\n\nFirst 10 rows of the ranks file:\n[('GEG', 0.00392156862745098), ('BUR', 0.00392156862745098), ('GRB', 0.00392156862745098), ('GTF', 0.00392156862745098), ('GRR', 0.00392156862745098), ('EUG', 0.00392156862745098), ('GSO', 0.00392156862745098), ('BTM', 0.00392156862745098), ('COD', 0.00392156862745098), ('FAR', 0.00392156862745098)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.d. [6pt] Recall the PageRank algorithm:\n\nSet  r=1n1n \nFor  i=1  to  I :\n   Set  r←1−βn1n+βMtr \nThe conts function below, is used to create an iterator that transfers from the ranks vector  r  to  Mr  , that is used in each iteration of the algorithm (in part 2 above).\nUsing the conts function, join the ranks to the links data, and create an RDD object called contrib, which stores for each node  j  the sum  ∑ni=1mijri , i.e. the contributions of the PageRank scores over all of the nodes that link to it.\nDisplay the top  10  values of the resulting RDD\n\nHint: First, use flatmap to obtain the contribution  mijri  for each link  i→j . Then, reduce to sum the contributions from all links going into the same destination node  j .\n\nNext, update the ranks vector using the resulting contrib according to the Page Rank algorithm, with  β=0.85 . This completes one iteration of the algorithm.\nDisplay the first  10  values of the resulting ranks RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39569433-10c9-4ce5-8bb1-8f6bc45d6e87"}}},{"cell_type":"code","source":["# Converts nodes iterator and ranks vec to ranks normalized by out-degree  \ndef conts(nodes, rank): \n    \"\"\"For each node in the graph calculate the number of connected  nodes, \n    and for each provide an node, adjusted rank by size of connected nodes:\n    \"\"\"\n    num_nodes = len(nodes)\n    for node in nodes:\n        yield (node, rank / num_nodes)\n\ncontrib = links.join(ranks).flatMap(lambda x: conts(x[1][0],x[1][1])) \nranks = contrib.reduceByKey(add).mapValues(lambda rank: rank*0.85 + 0.15/n)\n\nprint(\"First 10 values of the ranks RDD file:\")\nprint(ranks.take(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01fce53c-7473-4aad-ad5e-076ea45bc36a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"First 10 values of the ranks RDD file:\n[('GSP', 0.0010828977338659136), ('SRQ', 0.0008933302138109328), ('BZN', 0.0008545172310725196), ('GRB', 0.0007000804537888937), ('PIT', 0.0021256102937464993), ('AEX', 0.0006634933693757223), ('ORD', 0.07631369644235021), ('CHA', 0.0006916444453528852), ('GTR', 0.0006098803157626687), ('CAK', 0.0010119651891122173)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["First 10 values of the ranks RDD file:\n[('GSP', 0.0010828977338659136), ('SRQ', 0.0008933302138109328), ('BZN', 0.0008545172310725196), ('GRB', 0.0007000804537888937), ('PIT', 0.0021256102937464993), ('AEX', 0.0006634933693757223), ('ORD', 0.07631369644235021), ('CHA', 0.0006916444453528852), ('GTR', 0.0006098803157626687), ('CAK', 0.0010119651891122173)]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.e. [6pt]: The above code implemented one iteration of the PageRank algorithm.\nUse a loop to apply  50  iterations starting from the uniform initialization, and with a beta of  0.85 .\nShow the  10  airports with the highest PageRank score, along with their PageRank values, and City name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2125f522-970b-4621-a470-fe7069ff43d0"}}},{"cell_type":"code","source":["import pandas as pd\n#reset the 'ranks' and 'links' files:\norigins = spark.sql(\"select distinct origin from delays\")\nn = origins.count()\nranks = origins.rdd.map(lambda orig: (orig[0], 1/n))\ngrouped = ((spark.sql(\"select distinct origin,destination  from delays\")).rdd.map(lambda orig: (orig[0], orig[1]))).groupByKey()\nlinks = grouped.map(lambda orig: (orig[0], list(orig[1])))\n\nfor i in range(50):\n    contrib = links.join(ranks).flatMap(lambda x: conts(x[1][0],x[1][1])) \n    ranks = contrib.reduceByKey(add).mapValues(lambda rank: rank*0.85 + 0.15/n)\n\ntop_ranks = ranks.top(10, key=lambda val: val[1])\ntop_ranks = pd.DataFrame(top_ranks,columns = [\"Airport\",\"PageRank score\"])\nwith_city = spark.sql(\"SELECT origin,City FROM (SELECT t1.origin, t2.City FROM delays t1 JOIN airports t2 on t2.IATA = t1.origin) GROUP by origin,City\").toPandas()\n#to add the city name:\ntop_ranks[\"City\"] = top_ranks[\"Airport\"].map(with_city.set_index('origin')['City'])\n\nprint(\"The  10  airports with the highest PageRank score are: \")\ntop_ranks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a3e48e4-b730-46f9-b78a-1965d9c01c39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The  10  airports with the highest PageRank score are: \n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The  10  airports with the highest PageRank score are: \n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Airport</th>\n      <th>PageRank score</th>\n      <th>City</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ATL</td>\n      <td>0.036344</td>\n      <td>Atlanta</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ORD</td>\n      <td>0.032325</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DFW</td>\n      <td>0.032179</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DEN</td>\n      <td>0.025874</td>\n      <td>Denver</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IAH</td>\n      <td>0.021460</td>\n      <td>Houston</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MSP</td>\n      <td>0.018768</td>\n      <td>Minneapolis</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SLC</td>\n      <td>0.018729</td>\n      <td>Salt Lake City</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>DTW</td>\n      <td>0.017699</td>\n      <td>Detroit</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>LAX</td>\n      <td>0.015828</td>\n      <td>Los Angeles</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SFO</td>\n      <td>0.015545</td>\n      <td>San Francisco</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Airport</th>\n      <th>PageRank score</th>\n      <th>City</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ATL</td>\n      <td>0.036344</td>\n      <td>Atlanta</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ORD</td>\n      <td>0.032325</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DFW</td>\n      <td>0.032179</td>\n      <td>Dallas</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DEN</td>\n      <td>0.025874</td>\n      <td>Denver</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IAH</td>\n      <td>0.021460</td>\n      <td>Houston</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MSP</td>\n      <td>0.018768</td>\n      <td>Minneapolis</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SLC</td>\n      <td>0.018729</td>\n      <td>Salt Lake City</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>DTW</td>\n      <td>0.017699</td>\n      <td>Detroit</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>LAX</td>\n      <td>0.015828</td>\n      <td>Los Angeles</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SFO</td>\n      <td>0.015545</td>\n      <td>San Francisco</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.f [4pt] In this sub-question we run the PageRank algoithm on the much larger, wikiepdia dataset.\nLoad the wikipedia network dataset file created in Quesion 1.\nYour uploaded datasets can be accessed via: dbfs:/FileStore/shared_uploads/your.account.email@whatever.ending.you.have.com/\nYou should upload into two RDD objects using the sc.textFile command:\nOne, called keyvalue containing the nodes, uploaded from the keyvalue file.\nAnother, called transition containing the edges, uploaded from the file created in Question 1.b., in the format of one edge (two IDs) per line.\nDisplay the top  10  (nodes or edges) for each RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0811d9d-7fdf-4b6e-ad25-83f88564b227"}}},{"cell_type":"code","source":["keyvalue_path = \"dbfs:/FileStore/tables/keyvalue\"\ntransition_path = \"dbfs:/FileStore/tables/network\"\n\nkeyvalue = spark.sparkContext.textFile(keyvalue_path)\ntransition = spark.sparkContext.textFile(transition_path)\n\nprint(\"The top 10 nodes (by 'keyvalue' file) are:\")\nprint(keyvalue.take(10))\nprint(\" \")\nprint(\"The top 10 edges (by 'transition' file) are:\")\nprint(transition.take(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f2bf689-cfa2-4bf9-bb63-8c3553e21e39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The top 10 nodes (by 'keyvalue' file) are:\n['0\\thttps://en.wikipedia.org/wiki/Statistics', '1\\thttps://en.wikipedia.org/wiki/Category:Statistics', '2\\thttps://en.wikipedia.org/wiki/Portal:Mathematics', '3\\thttps://en.wikipedia.org/wiki/Normal_distribution', '4\\thttps://en.wikipedia.org/wiki/Scatter_plot', '5\\thttps://en.wikipedia.org/wiki/Iris_flower_data_set', '6\\thttps://en.wikipedia.org/wiki/Data', '7\\thttps://en.wikipedia.org/wiki/Statistical_model', '8\\thttps://en.wikipedia.org/wiki/Statistical_survey', '9\\thttps://en.wikipedia.org/wiki/Experimental_design']\n \nThe top 10 edges (by 'transition' file) are:\n['0 1', '0 2', '0 3', '0 4', '0 5', '0 6', '0 7', '0 8', '0 9', '0 10']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The top 10 nodes (by 'keyvalue' file) are:\n['0\\thttps://en.wikipedia.org/wiki/Statistics', '1\\thttps://en.wikipedia.org/wiki/Category:Statistics', '2\\thttps://en.wikipedia.org/wiki/Portal:Mathematics', '3\\thttps://en.wikipedia.org/wiki/Normal_distribution', '4\\thttps://en.wikipedia.org/wiki/Scatter_plot', '5\\thttps://en.wikipedia.org/wiki/Iris_flower_data_set', '6\\thttps://en.wikipedia.org/wiki/Data', '7\\thttps://en.wikipedia.org/wiki/Statistical_model', '8\\thttps://en.wikipedia.org/wiki/Statistical_survey', '9\\thttps://en.wikipedia.org/wiki/Experimental_design']\n \nThe top 10 edges (by 'transition' file) are:\n['0 1', '0 2', '0 3', '0 4', '0 5', '0 6', '0 7', '0 8', '0 9', '0 10']\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.g.[6pt] We want to avoid dangling pages when running the PageRank algorithm.\nTo do so, convert the transition RDD into a data-frame. In this data-frame, remove all links where the destination webpage does not appear as one of the source webpages (in similar to 1.c.).\nRepeat the process until you get a sub-network where every node is a source-node, i.e, has a positive out-degree (you may need to repeat the process more than once).\n\nRemark In order to access a Spark data-frame via Spark.sql, you need to first declare it. For example, if your data-frame is called transition_df, add to your code the line: transition_df.createOrReplaceTempView(\"transition_df\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25b690ac-2db2-4abc-9f2e-e66a9dfc6505"}}},{"cell_type":"code","source":["import pandas as pd\n\ntransition_df = transition.map(lambda x: x.split(\" \")).toDF().toPandas()\ntransition_df.columns = [\"source\",\"destination\"]\nspark_df = spark.createDataFrame(transition_df)\nspark_df.createOrReplaceTempView(\"spark_df\")\nprint(\"Before removal there were \"+str(transition_df.shape[0])+\" rows\")\nspark_df = spark.sql(\"select * from spark_df as a WHERE  EXISTS (select * from spark_df as b where b.source = a.destination)\")\ntransition_df = spark_df.toPandas()\nprint(\"After removal \"+str(transition_df.shape[0])+\" rows left\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54b4062e-5630-4fb1-89b1-db56c68b7e71"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Before removal there were 11874148 rows\nAfter removal 5459898 rows left\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Before removal there were 11874148 rows\nAfter removal 5459898 rows left\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["3.h.[6pt] Finally, run the PageRank algorithm on the wikipedia netowrk, with 10 iterations and  β=0.85 , and a uniform ranks vector initialization.\nYour implementation should be similar to the implementation for the flight delays dataset. Show the 20 wikipedia pages with the highest PageRank values you got (the url along with their PageRank scores)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be6fdd6c-62a7-4fd1-9f3a-9a3afedb4836"}}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql import functions\nimport pyspark.sql.functions as F\nfrom operator import add\n\n#create the ranks and links files:\nsources = spark.sql(\"select distinct source from spark_df\")\nn = sources.count()\nranks = sources.rdd.map(lambda source: (source[0], 1/n))\n\ngrouped = ((spark.sql(\"select distinct source,destination  from spark_df\")).rdd.map(lambda orig: (orig[0], orig[1]))).groupByKey()\nlinks = grouped.map(lambda orig: (orig[0], list(orig[1])))\n\ndef conts(nodes, rank): \n    \"\"\"For each node in the graph calculate the number of connected  nodes, \n    and for each provide an node, adjusted rank by size of connected nodes:\n    \"\"\"\n    num_nodes = len(nodes)\n    for node in nodes:\n        yield (node, rank / num_nodes)\n\nfor i in range(10):\n    contrib = links.join(ranks).flatMap(lambda x: conts(x[1][0],x[1][1])) \n    ranks = contrib.reduceByKey(add).mapValues(lambda rank: rank*0.85 + 0.15/n)\n\ntop_ranks = ranks.top(20, key=lambda val: val[1])\ntop_ranks = pd.DataFrame(top_ranks,columns = [\"page ID\",\"PageRank score\"])\n\n#to get the urls:\nkeyvalue_df = keyvalue.map(lambda x: x.split(\"\\t\")).toDF().toPandas()\nkeyvalue_df.columns = [\"ID\",\"URL\"]\n\ntop_ranks[\"URL\"] = top_ranks[\"page ID\"].map(keyvalue_df.set_index('ID')['URL'])\n\nprint(\"The 20 wikipedia pages with the highest PageRank score are: \")\ntop_ranks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"892ef31c-987b-4fe5-ab5c-8cc8d6b4a60a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The 20 wikipedia pages with the highest PageRank score are: \n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The 20 wikipedia pages with the highest PageRank score are: \n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page ID</th>\n      <th>PageRank score</th>\n      <th>URL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>454</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Special:SpecialP...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>448</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:Contents</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>445</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Special:MyTalk</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>453</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Help:Introduction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>447</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Main_Page</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>449</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Special:Random</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>452</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Help:Contents</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>450</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:About</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>451</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:Contac...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>442</td>\n      <td>0.002075</td>\n      <td>https://en.wikipedia.org/wiki/Help:Category</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>233</td>\n      <td>0.001018</td>\n      <td>https://en.wikipedia.org/wiki/ISBN_(identifier)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>235</td>\n      <td>0.000485</td>\n      <td>https://en.wikipedia.org/wiki/Doi_(identifier)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5245</td>\n      <td>0.000425</td>\n      <td>https://en.wikipedia.org/wiki/VIAF_(identifier)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5244</td>\n      <td>0.000294</td>\n      <td>https://en.wikipedia.org/wiki/ISNI_(identifier)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>247</td>\n      <td>0.000286</td>\n      <td>https://en.wikipedia.org/wiki/ISSN_(identifier)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>821</td>\n      <td>0.000283</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:Stub</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>249</td>\n      <td>0.000271</td>\n      <td>https://en.wikipedia.org/wiki/Wayback_Machine</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5246</td>\n      <td>0.000266</td>\n      <td>https://en.wikipedia.org/wiki/SUDOC_(identifier)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>236</td>\n      <td>0.000247</td>\n      <td>https://en.wikipedia.org/wiki/S2CID_(identifier)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>237</td>\n      <td>0.000215</td>\n      <td>https://en.wikipedia.org/wiki/JSTOR_(identifier)</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page ID</th>\n      <th>PageRank score</th>\n      <th>URL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>454</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Special:SpecialP...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>448</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:Contents</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>445</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Special:MyTalk</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>453</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Help:Introduction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>447</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Main_Page</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>449</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Special:Random</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>452</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Help:Contents</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>450</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:About</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>451</td>\n      <td>0.002127</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:Contac...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>442</td>\n      <td>0.002075</td>\n      <td>https://en.wikipedia.org/wiki/Help:Category</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>233</td>\n      <td>0.001018</td>\n      <td>https://en.wikipedia.org/wiki/ISBN_(identifier)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>235</td>\n      <td>0.000485</td>\n      <td>https://en.wikipedia.org/wiki/Doi_(identifier)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5245</td>\n      <td>0.000425</td>\n      <td>https://en.wikipedia.org/wiki/VIAF_(identifier)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5244</td>\n      <td>0.000294</td>\n      <td>https://en.wikipedia.org/wiki/ISNI_(identifier)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>247</td>\n      <td>0.000286</td>\n      <td>https://en.wikipedia.org/wiki/ISSN_(identifier)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>821</td>\n      <td>0.000283</td>\n      <td>https://en.wikipedia.org/wiki/Wikipedia:Stub</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>249</td>\n      <td>0.000271</td>\n      <td>https://en.wikipedia.org/wiki/Wayback_Machine</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5246</td>\n      <td>0.000266</td>\n      <td>https://en.wikipedia.org/wiki/SUDOC_(identifier)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>236</td>\n      <td>0.000247</td>\n      <td>https://en.wikipedia.org/wiki/S2CID_(identifier)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>237</td>\n      <td>0.000215</td>\n      <td>https://en.wikipedia.org/wiki/JSTOR_(identifier)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Q3","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3509284538179022}},"nbformat":4,"nbformat_minor":0}
